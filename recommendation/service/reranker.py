from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

import numpy as np

from ..data.data_loader import MongoDataLoader
from ..models.data_models import RecommendationResult, UserProfile
from ..rl.state_builder import build_candidate_features
from ..rl.bandit_policy import SimpleBanditModel, DEFAULT_MODEL_PATH

logger = logging.getLogger(__name__)

try:
    import torch
except ImportError:
    torch = None  # torch ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œëŠ” RL ê¸°ëŠ¥ ë¹„í™œì„±í™”


@dataclass
class RerankConfig:
    model_path: Path = Path(DEFAULT_MODEL_PATH)


class BanditPolicyWrapper:
    """
    SimpleBanditModelì„ ê°ì‹¸ëŠ” ë˜í¼.
    - ì²˜ìŒ í˜¸ì¶œ ì‹œì—ë§Œ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ë©”ëª¨ë¦¬ì— ìœ ì§€
    - ëª¨ë¸ íŒŒì¼ì´ ì—†ê±°ë‚˜ torch ë¯¸ì„¤ì¹˜ë©´ None ìƒíƒœë¡œ ë‘ê³ , ê·¸ ê²½ìš° rule-basedë§Œ ì‚¬ìš©
    """
    def __init__(self, config: Optional[RerankConfig] = None) -> None:
        self.config = config or RerankConfig()
        self._model: Optional[SimpleBanditModel] = None

    def _ensure_model(self, input_dim: int) -> None:
        if self._model is not None:
            return

        # torch ì—†ëŠ” í™˜ê²½ì´ë©´ ê·¸ëŒ€ë¡œ í¬ê¸°
        if torch is None:
            logger.warning("[RL Reranker] âš ï¸ torch ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ RL rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            return

        if not self.config.model_path.exists():
            logger.warning(f"[RL Reranker] âš ï¸ RL ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.config.model_path}")
            return

        logger.info(f"[RL Reranker] ğŸ§  RL ëª¨ë¸ ë¡œë”©: {self.config.model_path} (input_dim={input_dim})")
        model = SimpleBanditModel(input_dim=input_dim)
        state = torch.load(self.config.model_path, map_location="cpu")
        model.load_state_dict(state)
        model.eval()
        self._model = model
        logger.info("[RL Reranker] âœ… RL ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def predict_scores(self, X: np.ndarray) -> Optional[np.ndarray]:
        """
        X: (N, D) feature matrix
        return: (N,) ì˜ˆì¸¡ ì ìˆ˜ or None (ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ ì‹œ)
        """
        if X.size == 0:
            return None

        self._ensure_model(input_dim=X.shape[1])
        if self._model is None or torch is None:
            return None

        with torch.no_grad():
            t = torch.from_numpy(X).float()
            y = self._model(t).squeeze(-1).cpu().numpy()
        
        logger.info(f"[RL Reranker] ğŸ² RL ì ìˆ˜ ì˜ˆì¸¡ ì™„ë£Œ: min={y.min():.4f}, max={y.max():.4f}, mean={y.mean():.4f}")
        return y


class RLBanditReranker:
    """
    Rule-based í›„ë³´êµ°(List[RecommendationResult])ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„
    Contextual Bandit ì •ì±…ìœ¼ë¡œ rerank í›„ top_kê°œë¥¼ ë°˜í™˜.
    """
    def __init__(self, loader: Optional[MongoDataLoader] = None):
        self.loader = loader or MongoDataLoader()
        self.policy = BanditPolicyWrapper()

    def rerank(
        self,
        user_id: int,
        candidates: List[RecommendationResult],
        top_k: int = 6,
    ) -> List[RecommendationResult]:
        """
        candidates: RuleBasedRecommenderê°€ ë§Œë“  ìƒìœ„ Nê°œ (ì˜ˆ: 100ê°œ)
        return: RL ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•œ ìƒìœ„ top_k
        """
        if not candidates:
            return []

        logger.info(f"[RL Reranker] ğŸ“¥ Reranking ì‹œì‘: {len(candidates)}ê°œ í›„ë³´ â†’ top {top_k}")

        # 1) UserProfile ë¡œë”©
        profile: UserProfile = self.loader.build_user_profile(user_id)

        # 2) í›„ë³´ ë…¼ë¬¸ë“¤ feature matrix ìƒì„±
        papers = [c.paper for c in candidates]
        X, paper_ids, _feat_dicts = build_candidate_features(profile, papers)
        logger.info(f"[RL Reranker] ğŸ“Š Feature matrix ìƒì„±: shape={X.shape}")

        # 3) RL ì •ì±…ìœ¼ë¡œ ì ìˆ˜ ì˜ˆì¸¡
        rl_scores = self.policy.predict_scores(X)

        # RL ì‚¬ìš© ë¶ˆê°€(troch ë¯¸ì„¤ì¹˜, ëª¨ë¸ ì—†ìŒ ë“±) â†’ rule-based ìˆœì„œ ê·¸ëŒ€ë¡œ top_k
        if rl_scores is None:
            logger.warning("[RL Reranker] âš ï¸ RL ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ â†’ rule-based ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            return candidates[:top_k]

        logger.info("[RL Reranker] âœ… RL ëª¨ë¸ í™œì„±í™” â†’ RL ì ìˆ˜ë¡œ reranking")

        # 4) í›„ë³´ë“¤ì— RL ì ìˆ˜ ì ìš©
        reranked: List[RecommendationResult] = []
        for c, rl_s in zip(candidates, rl_scores):
            # ê¸°ì¡´ rule-based scoreë¥¼ ë³´ì¡´í•˜ê³ , scoreë¥¼ RL ì ìˆ˜ë¡œ ë®ì–´ì”Œì›€
            rule_score = c.score
            c.features = dict(c.features or {})
            c.features["rule_score"] = float(rule_score)
            c.features["rl_score"] = float(rl_s)
            c.score = float(rl_s)
            reranked.append(c)

        # 5) RL ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ top_k
        reranked.sort(key=lambda r: r.score, reverse=True)
        
        logger.info(f"[RL Reranker] ğŸ“¤ Reranking ì™„ë£Œ: {len(reranked[:top_k])}ê°œ ë°˜í™˜")
        return reranked[:top_k]