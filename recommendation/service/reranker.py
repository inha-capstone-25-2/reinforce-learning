from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

import numpy as np

from ..data.data_loader import MongoDataLoader
from ..models.data_models import RecommendationResult, UserProfile
from ..rl.state_builder import build_candidate_features
from ..rl.bandit_policy import SimpleBanditModel, DEFAULT_MODEL_PATH

logger = logging.getLogger(__name__)

try:
    import torch
except ImportError:
    torch = None  # torch ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œëŠ” RL ê¸°ëŠ¥ ë¹„í™œì„±í™”


@dataclass
class RerankConfig:
    model_path: Path = Path(DEFAULT_MODEL_PATH)


class BanditPolicyWrapper:
    """
    SimpleBanditModelì„ ê°ì‹¸ëŠ” ëž˜í¼.
    - ì²˜ìŒ í˜¸ì¶œ ì‹œì—ë§Œ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ë©”ëª¨ë¦¬ì— ìœ ì§€
    - ëª¨ë¸ íŒŒì¼ì´ ì—†ê±°ë‚˜ torch ë¯¸ì„¤ì¹˜ë©´ None ìƒíƒœë¡œ ë‘ê³ , ê·¸ ê²½ìš° rule-basedë§Œ ì‚¬ìš©
    """
    def __init__(self, config: Optional[RerankConfig] = None) -> None:
        self.config = config or RerankConfig()
        self._model: Optional[SimpleBanditModel] = None

    def _ensure_model(self, input_dim: int) -> None:
        if self._model is not None:
            return

        # torch ì—†ëŠ” í™˜ê²½ì´ë©´ ê·¸ëŒ€ë¡œ í¬ê¸°
        if torch is None:
            logger.warning("[RL Reranker] âš ï¸ torch ê°€ ì„¤ì¹˜ë˜ì–´ ìžˆì§€ ì•Šì•„ RL rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            return

        if not self.config.model_path.exists():
            logger.warning(f"[RL Reranker] âš ï¸ RL ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.config.model_path}")
            return

        logger.info(f"[RL Reranker] ðŸ§  RL ëª¨ë¸ ë¡œë”©: {self.config.model_path} (input_dim={input_dim})")
        model = SimpleBanditModel(input_dim=input_dim)
        state = torch.load(self.config.model_path, map_location="cpu")
        model.load_state_dict(state)
        model.eval()
        self._model = model
        logger.info("[RL Reranker] âœ… RL ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def predict_scores(self, X: np.ndarray) -> Optional[np.ndarray]:
        """
        X: (N, D) feature matrix
        return: (N,) ì˜ˆì¸¡ ì ìˆ˜ or None (ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ ì‹œ)
        """
        if X.size == 0:
            return None

        self._ensure_model(input_dim=X.shape[1])
        if self._model is None or torch is None:
            return None

        with torch.no_grad():
            t = torch.from_numpy(X).float()
            y = self._model(t).squeeze(-1).cpu().numpy()
        
        logger.info(f"[RL Reranker] ðŸŽ² RL ì ìˆ˜ ì˜ˆì¸¡ ì™„ë£Œ: min={y.min():.4f}, max={y.max():.4f}, mean={y.mean():.4f}")
        return y


class RLBanditReranker:
    """
    Rule-based í›„ë³´êµ°(List[RecommendationResult])ì„ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„
    Contextual Bandit ì •ì±…ìœ¼ë¡œ rerank í›„ top_kê°œë¥¼ ë°˜í™˜.
    """
    def __init__(self, loader: Optional[MongoDataLoader] = None):
        self.loader = loader or MongoDataLoader()
        self.policy = BanditPolicyWrapper()

    def rerank(
        self,
        user_id: int,
        candidates: List[RecommendationResult],
        top_k: int = 6,
    ) -> List[RecommendationResult]:
        """
        candidates: RuleBasedRecommenderê°€ ë§Œë“  ìƒìœ„ Nê°œ (ì˜ˆ: 100ê°œ)
        return: RL ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•œ ìƒìœ„ top_k
        """
        if not candidates:
            return []

        logger.info(f"[RL Reranker] ðŸ“¥ Reranking ì‹œìž‘: {len(candidates)}ê°œ í›„ë³´ â†’ top {top_k}")

        # 1) UserProfile ë¡œë”©
        profile: UserProfile = self.loader.build_user_profile(user_id)

        # 2) í›„ë³´ ë…¼ë¬¸ë“¤ feature matrix ìƒì„±
        papers = [c.paper for c in candidates]
        X, paper_ids, _feat_dicts = build_candidate_features(profile, papers)
        logger.info(f"[RL Reranker] ðŸ“Š Feature matrix ìƒì„±: shape={X.shape}")

        # 3) RL ì •ì±…ìœ¼ë¡œ ì ìˆ˜ ì˜ˆì¸¡
        rl_scores = self.policy.predict_scores(X)

        # RL ì‚¬ìš© ë¶ˆê°€(troch ë¯¸ì„¤ì¹˜, ëª¨ë¸ ì—†ìŒ ë“±) â†’ rule-based ìˆœì„œ ê·¸ëŒ€ë¡œ top_k
        if rl_scores is None:
            logger.warning("[RL Reranker] âš ï¸ RL ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ â†’ rule-based ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            return candidates[:top_k]

        logger.info("[RL Reranker] âœ… RL ëª¨ë¸ í™œì„±í™” â†’ RL ì ìˆ˜ë¡œ reranking")

        # 4) í›„ë³´ë“¤ì— RL ì ìˆ˜ ì ìš©
        reranked: List[RecommendationResult] = []
        for c, rl_s in zip(candidates, rl_scores):
            # ê¸°ì¡´ rule-based scoreë¥¼ ë³´ì¡´í•˜ê³ , scoreë¥¼ RL ì ìˆ˜ë¡œ ë®ì–´ì”Œì›€
            rule_score = c.score
            c.features = dict(c.features or {})
            c.features["rule_score"] = float(rule_score)
            c.features["rl_score"] = float(rl_s)
            final_score = 0.6 * rl_s + 0.4 * rule_score
            c.score = final_score
            reranked.append(c)

        # 5) RL ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ top_k 
        reranked.sort(key=lambda r: r.score, reverse=True)

        # ë‹¤ì–‘ì„±ì´ ë„ˆë¬´ ì—†ëŠ” ê´€ê³„ë¡œ ìˆ˜ì •!
        final: List[RecommendationResult] = []
        used_categories = set()

        """
        ì´ë¯¸ ì„ íƒëœ ê²ƒë“¤ê³¼ ì¹´í…Œê³ ë¦¬ê°€ ê²¹ì¹˜ë©´ íŒ¨ë„í‹° (ë˜ëŠ” ìŠ¤í‚µ)
        ë„ˆë¬´ ë¹¡ì„¸ê²Œ ìŠ¤í‚µí•˜ë©´ ì¶”ì²œì´ ë¹„ì–´ë²„ë¦´ ìˆ˜ ìžˆìœ¼ë‹ˆê¹Œ ê°€ëŠ¥í•˜ë©´
        ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ìš°ì„ ìœ¼ë¡œ êµ¬ì„±. ì™œëƒë©´ í›„ë³´êµ°ì—ì„œ ì¶”ì²œëœê±¸ rerankí•˜ëŠ”ê±°ë¼
        """
        for cand in reranked:
            cats = set(getattr(cand.paper, "categories", []) or [])
            if used_categories and cats & used_categories and len(final) >= 3:#ì•žìª½ 3ê°œëŠ” ê·¸ëƒ¥ ë‘ê³  ì´í›„ë¶€í„°ëŠ” ê²¹ì¹˜ëŠ” ê±´ í•œ ë²ˆ ê±´ë„ˆë›°ëŠ” ì‹
                continue

            final.append(cand)
            used_categories.update(cats)
            if len(final) >= top_k:
                break

        if len(final) < top_k:
            for cand in reranked:
                if cand in final:
                    continue
                final.append(cand)
                if len(final) >= top_k:
                    break  

        return final[:top_k]
